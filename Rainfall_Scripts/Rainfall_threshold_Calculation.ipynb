{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "from matplotlib.dates import DateFormatter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = netCDF4.Dataset('./precipitation/era5_hourly/precip.nc', 'r')\n",
    "time = dataset.variables['time'][:]\n",
    "time_units = dataset.variables['time'].units\n",
    "time_calendar = dataset.variables['time'].calendar\n",
    "dates = netCDF4.num2date(time, units=time_units, calendar=time_calendar)\n",
    "dates = [datetime(d.year, d.month, d.day, d.hour, d.minute, d.second) for d in dates]\n",
    "precipitation = dataset.variables['precip'][:]  \n",
    "# Access the latitude and longitude variables\n",
    "latitudes = dataset.variables['latitude'][:]\n",
    "longitudes = dataset.variables['longitude'][:]\n",
    "\n",
    "# Load the inventory\n",
    "gdf = gpd.read_file('./inventory/landslides_liguria.shp')\n",
    "gdf['utc_date'] = pd.to_datetime(gdf['utc_date'], format=\"%d/%m/%Y %H:%M\")\n",
    "\n",
    "\n",
    "pixel_events = {}\n",
    "\n",
    "# Loop through each event in the inventory\n",
    "for index, row in gdf.iterrows():\n",
    "    event_date = row['utc_date']\n",
    "    location = row['geometry']\n",
    "    if isinstance(location, Point):\n",
    "        lat, lon = location.y, location.x\n",
    "        \n",
    "        lat_index = np.argmin(np.abs(latitudes - lat))\n",
    "        lon_index = np.argmin(np.abs(longitudes - lon))\n",
    "\n",
    "        pixel_key = (lat_index, lon_index)\n",
    "        if pixel_key not in pixel_events:\n",
    "            pixel_events[pixel_key] = []\n",
    "        pixel_events[pixel_key].append(event_date)\n",
    "\n",
    "\n",
    "# Function to filter rainfall events\n",
    "def filter_rainfall_events(precipitation, dates, threshold=0.2, min_duration=2, max_gap=24):\n",
    "    events = []\n",
    "    event = []\n",
    "    event_dates = []\n",
    "    gap_counter = 0  # Counter for hours below the threshold\n",
    "\n",
    "    for i in range(len(dates)):\n",
    "        if precipitation[i] > threshold:\n",
    "            event.append(precipitation[i])\n",
    "            event_dates.append(dates[i])\n",
    "            gap_counter = 0  # Reset the gap counter when rainfall is above the threshold\n",
    "        else:\n",
    "            if len(event) > 0:\n",
    "                gap_counter += 1\n",
    "                if gap_counter > max_gap:\n",
    "                    # End the current event if the gap exceeds the allowed maximum\n",
    "                    if len(event) > 0:\n",
    "                        events.append((event, event_dates))\n",
    "                    event = []\n",
    "                    event_dates = []\n",
    "                    gap_counter = 0  # Reset the gap counter\n",
    "\n",
    "    # Add the last event if it exists\n",
    "    if len(event) > 0:\n",
    "        events.append((event, event_dates))\n",
    "\n",
    "    # Filter events based on minimum duration\n",
    "    filtered_events = [(e, d) for e, d in events if len(e) > min_duration]\n",
    "    return filtered_events\n",
    "\n",
    "\n",
    "'''\n",
    "def filter_rainfall_events(precipitation, dates, threshold=0.2, min_duration=2):\n",
    "    events = []\n",
    "    event = []\n",
    "    event_dates = []\n",
    "    dry_season = range(6, 10)  # June to September\n",
    "    wet_season = chain(range(10, 13), range(1, 6))  # October to May\n",
    "\n",
    "    for i in range(len(dates)):\n",
    "        if precipitation[i] > threshold:\n",
    "            event.append(precipitation[i])\n",
    "            event_dates.append(dates[i])\n",
    "        else:\n",
    "            if len(event) > 0:\n",
    "                events.append((event, event_dates))\n",
    "                event = []\n",
    "                event_dates = []\n",
    "\n",
    "    if len(event) > 0:\n",
    "        events.append((event, event_dates))\n",
    "\n",
    "    filtered_events = []\n",
    "    for event, event_dates in events:\n",
    "        start_month = event_dates[0].month\n",
    "        end_month = event_dates[-1].month\n",
    "        if (start_month in dry_season and end_month in dry_season and \n",
    "            all((event_dates[i+1] - event_dates[i]).total_seconds() / 3600 < 48 for i in range(len(event_dates) - 1))):\n",
    "            filtered_events.append((event, event_dates))\n",
    "        elif (start_month in wet_season and end_month in wet_season and \n",
    "              all((event_dates[i+1] - event_dates[i]).total_seconds() / 3600 < 96 for i in range(len(event_dates) - 1))):\n",
    "            filtered_events.append((event, event_dates))\n",
    "\n",
    "    #print(\"Events:\", events)\n",
    "    #print(\"Filtered Events:\", filtered_events)\n",
    "    return [e for e in filtered_events if len(e[0]) > min_duration]\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "dataset.close()\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "pixel_dataframes = {}\n",
    "\n",
    "for index, row in gdf.iterrows():\n",
    "    event_date = row['utc_date']\n",
    "    location = row['geometry']\n",
    "    if isinstance(location, Point):\n",
    "        lat, lon = location.y, location.x\n",
    "        \n",
    "        lat_index = np.argmin(np.abs(latitudes - lat))\n",
    "        lon_index = np.argmin(np.abs(longitudes - lon))\n",
    "\n",
    "        pixel_key = (lat_index, lon_index)\n",
    "        if pixel_key not in pixel_events:\n",
    "            pixel_events[pixel_key] = []\n",
    "        pixel_events[pixel_key].append(event_date)\n",
    "        \n",
    "\n",
    "\n",
    "for (lat_index, lon_index), gdf_event_dates in pixel_events.items():\n",
    "    pixel_precipitation = precipitation[:, lat_index, lon_index]\n",
    "    filtered_events = filter_rainfall_events(pixel_precipitation, dates)\n",
    "    \n",
    "    # Create a DataFrame for the pixel\n",
    "    data = []\n",
    "\n",
    "    for event, event_dates in filtered_events:\n",
    "        start_time = event_dates[0]\n",
    "        end_time = event_dates[-1]\n",
    "        duration = (end_time - start_time).total_seconds() / 3600  # duration in hours\n",
    "        rainfall_intensity = np.mean(event)\n",
    "        cumulative_rf = np.sum(event)\n",
    "\n",
    "        \n",
    "        LS_date = None  \n",
    "        #Landslide_id = None\n",
    "        for gdf_event_date in gdf_event_dates:\n",
    "            \n",
    "            \n",
    "        \n",
    "            # Check if the landslide event occurs within the rainfall event or up to 48 hours after\n",
    "            if start_time <= gdf_event_date <= end_time + timedelta(hours=48):\n",
    "                LS_date = gdf_event_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                #Landslide_id = gdf_id\n",
    "                break  \n",
    "            \n",
    "\n",
    "        occurrences = 1 if LS_date else 0  # Set occurrences to 1 if LS_date is found, otherwise 0\n",
    "        data.append([LS_date,  start_time, end_time, duration, rainfall_intensity, cumulative_rf, occurrences, event])  # Add event (rainfall values) as a list\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['LS_date',  'start_time', 'end_time', 'duration', 'rainfall_intensity', 'Cumulative_RF', 'occurrences', 'rainfall_timeseries'])\n",
    "    pixel_dataframes[(lat_index, lon_index)] = df\n",
    "\n",
    "total_occurrences = 0\n",
    "# Print the DataFrames for each pixel\n",
    "for pixel, df in pixel_dataframes.items():\n",
    "    print(f\"DataFrame for pixel {pixel}:\")\n",
    "    print(df['occurrences'].sum())\n",
    "    total_occurrences += df['occurrences'].sum()\n",
    "    #display(df)\n",
    "print(f\"Total occurrences for all pixels: {total_occurrences}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "plots_shown = 0\n",
    "max_plots_to_show = 10  # Limit the number of plots to display to 10\n",
    "\n",
    "# Iterate through each pixel's DataFrame\n",
    "for pixel, df in pixel_dataframes.items():\n",
    "   \n",
    "    filtered_df = df[df['occurrences'] == 1]\n",
    "    \n",
    "    \n",
    "    print(f\"Rows with occurrences == 1 for pixel {pixel}:\")\n",
    "    display(filtered_df)\n",
    "    \n",
    "    # Iterate through the filtered rows\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        start_time = row['start_time']\n",
    "        end_time = row['end_time']\n",
    "        time_steps = pd.date_range(start=start_time, end=end_time, freq='H')  # One-hour intervals\n",
    "        \n",
    "        # Convert LS_date to pandas.Timestamp\n",
    "        LS_date = pd.to_datetime(row['LS_date'])\n",
    "        \n",
    "        # Debugging: Print start_time, end_time, and LS_date\n",
    "        print(f\"Pixel: {pixel}, Start: {start_time}, End: {end_time}, LS_date: {LS_date}\")\n",
    "        \n",
    "        # Ensure rainfall_timeseries is valid\n",
    "        rainfall_timeseries = row['rainfall_timeseries']\n",
    "        if not isinstance(rainfall_timeseries, (list, pd.Series)):\n",
    "            print(f\"Invalid rainfall_timeseries for pixel {pixel}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Ensure time_steps and rainfall_timeseries have the same length\n",
    "        if len(time_steps) != len(rainfall_timeseries):\n",
    "            print(f\"Length mismatch for pixel {pixel}: time_steps ({len(time_steps)}) vs rainfall_timeseries ({len(rainfall_timeseries)}). Adjusting...\")\n",
    "            time_steps = time_steps[:len(rainfall_timeseries)]  # Trim time_steps to match rainfall_timeseries\n",
    "        \n",
    "        # Find the index of LS_date in time_steps\n",
    "        if LS_date in time_steps:\n",
    "            ls_index = time_steps.get_loc(LS_date)\n",
    "        else:\n",
    "            print(f\"LS_date ({LS_date}) not found in time_steps for pixel {pixel}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Check if all points before LS_date are less than 0.5\n",
    "        if all(value < 0.5 for value in rainfall_timeseries[:ls_index]):\n",
    "            print(f\"All points before LS_date are less than 0.5 for pixel {pixel}, deleting row...\")\n",
    "            filtered_df.drop(index, inplace=True)  # Delete the row from the DataFrame\n",
    "            continue  \n",
    "        \n",
    "        \n",
    "        if plots_shown < max_plots_to_show:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(time_steps, rainfall_timeseries, marker='o', label=f\"Pixel: {pixel}, Start: {start_time}\")\n",
    "            \n",
    "            # Add LS_date as a red dashed vertical line (always plot it)\n",
    "            plt.axvline(x=LS_date, color='red', linestyle='--', label='LS_date')\n",
    "            \n",
    "            # Add a note if LS_date is outside the range\n",
    "            if not (start_time <= LS_date <= end_time):\n",
    "                print(f\"Note: LS_date ({LS_date}) is outside the range of start_time ({start_time}) and end_time ({end_time}).\")\n",
    "            \n",
    "            plt.title(f\"Rainfall Timeseries for Pixel {pixel}\")\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Rainfall (mm)\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "            \n",
    "            plots_shown += 1  # Increment the plot counter\n",
    "    \n",
    "    # Continue processing all rows, even if the plot limit is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_scatter(pixel_dataframes,a):\n",
    "    for pixel, df in pixel_dataframes.items():\n",
    "        # Filter data based on occurrences\n",
    "        df_occurrences_0 = df[df['occurrences'] == 0]\n",
    "        df_occurrences_above_0 = df[df['occurrences'] > 0]\n",
    "\n",
    "        \n",
    "        plt.scatter(df_occurrences_0['duration'], df_occurrences_0[a], color='blue', alpha=0.2, label='Non Occurrences')\n",
    "        plt.scatter(df_occurrences_above_0['duration'], df_occurrences_above_0[a], color='red', alpha=0.5, label='Occurrences')\n",
    "\n",
    "        \n",
    "        plt.title(f'Scatter Plot for {pixel}')\n",
    "        plt.xlabel('Duration (log scale)')\n",
    "        plt.ylabel(a)\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "a = 'rainfall_intensity'\n",
    "#a= 'Cumulative_RF'\n",
    "plot_scatter(pixel_dataframes,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the whole area 01\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_combined_scatter(pixel_dataframes, a):\n",
    "    # Validate and reset index for each DataFrame\n",
    "    cleaned_dataframes = []\n",
    "    for pixel, df in pixel_dataframes.items():\n",
    "        print(f\"Processing pixel: {pixel}\")\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError(f\"Value for pixel '{pixel}' is not a DataFrame.\")\n",
    "        if df.empty:\n",
    "            print(f\"Warning: DataFrame for pixel '{pixel}' is empty and will be skipped.\")\n",
    "            continue\n",
    "        # Debugging: Print DataFrame shape and index length\n",
    "        print(f\"DataFrame shape: {df.shape}, Index length: {len(df.index)}\")\n",
    "        # Ensure the index matches the data length\n",
    "        if len(df.index) != len(df):\n",
    "            print(f\"Warning: DataFrame for pixel '{pixel}' has a mismatched index. Resetting index.\")\n",
    "            df = df.reset_index(drop=True)\n",
    "        # Convert pixel to string and assign it as a column\n",
    "        cleaned_dataframes.append(df.assign(pixel=str(pixel)))\n",
    "\n",
    "    # Merge all cleaned DataFrames into one\n",
    "    if not cleaned_dataframes:\n",
    "        raise ValueError(\"No valid DataFrames to plot.\")\n",
    "    combined_df = pd.concat(cleaned_dataframes, ignore_index=True)\n",
    "    cd_output_file = \"Rainfall_Events_2002-2022.xlsx\"\n",
    "    combined_df.to_excel(cd_output_file, index=False)\n",
    "\n",
    "    # Filter data based on occurrences\n",
    "    df_occurrences_0 = combined_df[combined_df['occurrences'] == 0]\n",
    "    df_occurrences_above_0 = combined_df[combined_df['occurrences'] > 0]\n",
    "\n",
    "    \n",
    "    plt.scatter(df_occurrences_0['duration'], df_occurrences_0[a], color='blue', alpha=0.2, label='Non Occurrences')\n",
    "    plt.scatter(df_occurrences_above_0['duration'], df_occurrences_above_0[a], color='red', alpha=0.5, label='Occurrences')\n",
    "\n",
    "    \n",
    "    plt.title('Combined Scatter Plot')\n",
    "    plt.xlabel('Duration (log scale)')\n",
    "    plt.ylabel(a)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#a = 'rainfall_intensity'\n",
    "a = 'Cumulative_RF'\n",
    "plot_combined_scatter(pixel_dataframes, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the whole area 02\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calculate_rainfall_threshold(df, RF_metric):\n",
    "    percentiles = np.arange(0, 101, 5)\n",
    "    thresholds = np.percentile(df[RF_metric], percentiles)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        df['Prediction'] = np.where(df[RF_metric] >= threshold, 1, 0)\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(df['occurrences'], df['Prediction']).ravel()\n",
    "        \n",
    "        # Calculate TPR and TNR\n",
    "        tpr = tp / (tp + fn)\n",
    "        tnr = tn / (tn + fp)\n",
    "        \n",
    "        # Calculate Youden's Index\n",
    "        youden_index = tpr + tnr - 1\n",
    "        \n",
    "        results.append({\n",
    "            'Threshold': threshold,\n",
    "            'TPR': tpr,\n",
    "            'TNR': tnr,\n",
    "            'Youden_Index': youden_index\n",
    "        })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Step 3: Determine the optimal threshold based on Youden's Index\n",
    "    optimal_threshold = results_df.loc[results_df['Youden_Index'].idxmax()]\n",
    "\n",
    "    # Output the optimal threshold\n",
    "    print(\"Optimal Rainfall Threshold (RT):\", optimal_threshold['Threshold'])\n",
    "    print(\"True Positive Rate (TPR):\", optimal_threshold['TPR'])\n",
    "    print(\"True Negative Rate (TNR):\", optimal_threshold['TNR'])\n",
    "    print(\"Youden's Index:\", optimal_threshold['Youden_Index'])\n",
    "    return results_df\n",
    "\n",
    "def combine_and_calculate_threshold(pixel_dataframes, RF_metric):\n",
    "    # Combine all dataframes into one\n",
    "    combined_df = pd.concat(\n",
    "        [df.reset_index(drop=True).assign(pixel=str(pixel)) for pixel, df in pixel_dataframes.items()],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    if 'occurrences' not in combined_df.columns or RF_metric not in combined_df.columns:\n",
    "        raise ValueError(f\"The combined dataframe must contain 'occurrences' and '{RF_metric}' columns.\")\n",
    "\n",
    "    # Calculate the rainfall threshold\n",
    "    results_df = calculate_rainfall_threshold(combined_df, RF_metric)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "RF_metric = 'Cumulative_RF'\n",
    "optimal_values = combine_and_calculate_threshold(pixel_dataframes, RF_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def calculate_rainfall_threshold(df,RF_metric):\n",
    "# Step 1: Vary the percentile threshold\n",
    "    percentiles = np.arange(0, 101, 5)\n",
    "    thresholds = np.percentile(df[RF_metric], percentiles)\n",
    "\n",
    "# Step 2: Calculate TPR, TNR, and Youden's Index\n",
    "    results = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        df['Prediction'] = np.where(df[RF_metric] >= threshold, 1, 0)\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(df['occurrences'], df['Prediction']).ravel()\n",
    "        \n",
    "        # Calculate TPR and TNR\n",
    "        tpr = tp / (tp + fn)\n",
    "        tnr = tn / (tn + fp)\n",
    "        \n",
    "        # Calculate Youden's Index\n",
    "        youden_index = tpr + tnr - 1\n",
    "        \n",
    "        results.append({\n",
    "            'Threshold': threshold,\n",
    "            'TPR': tpr,\n",
    "            'TNR': tnr,\n",
    "            'Youden_Index': youden_index\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Step 3: Determine the optimal threshold based on Youden's Index\n",
    "    optimal_threshold = results_df.loc[results_df['Youden_Index'].idxmax()]\n",
    "\n",
    "    # Output the optimal threshold\n",
    "    #print(\"Optimal Rainfall Threshold (RT):\", optimal_threshold['Threshold'])\n",
    "    #print(\"True Positive Rate (TPR):\", optimal_threshold['TPR'])\n",
    "    #print(\"True Negative Rate (TNR):\", optimal_threshold['TNR'])\n",
    "    #print(\"Youden's Index:\", optimal_threshold['Youden_Index'])\n",
    "    #return results_df\n",
    "    return {\n",
    "        'Optimal_Threshold': optimal_threshold['Threshold'],\n",
    "        'Optimal_Youden_Index': optimal_threshold['Youden_Index'],\n",
    "        \"True Positive Rate (TPR)\": optimal_threshold['TPR'],\n",
    "        \"True Negative Rate (TNR)\": optimal_threshold['TNR'],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "summary_table = []\n",
    "\n",
    "for pixel, df in pixel_dataframes.items():\n",
    "    total_occurrences = df['occurrences'].sum()\n",
    "    total_observations = df.shape[0]\n",
    "    occurrence_rate = (total_occurrences / total_observations) * 100\n",
    "    \n",
    "    # Calculate optimal threshold and Youden's Index\n",
    "    optimal_values = calculate_rainfall_threshold(df, 'Cumulative_RF')\n",
    "    \n",
    "    # Append results to the summary table\n",
    "    summary_table.append({\n",
    "        'Pixel': pixel,\n",
    "        'Total_Observations': total_observations,\n",
    "        'Occurrence_Rate (%)': occurrence_rate,\n",
    "        \"True Positive Rate (TPR)\": optimal_values['True Positive Rate (TPR)'],\n",
    "        \"True Negative Rate (TPR)\": optimal_values['True Negative Rate (TNR)'],\n",
    "        'Optimal_Youden_Index': optimal_values['Optimal_Youden_Index'],\n",
    "        'Rainfall_Threshold': optimal_values['Optimal_Threshold']\n",
    "    })\n",
    "\n",
    "# Convert summary table to DataFrame and display\n",
    "summary_df = pd.DataFrame(summary_table)\n",
    "display(summary_df)\n",
    "output_file = \"Rainfall_Summary.xlsx\"\n",
    "summary_df.to_excel(output_file, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_cleaned = gpd.read_file(f'Example_Susceptibility_25_11_2016/Centroids_RF_sampled_ver01.shp')\n",
    "#shapefile_cleaned['geometry'] = shapefile_cleaned.geometry.buffer(0.000256, cap_style=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "point1 = shapefile_cleaned.geometry.iloc[0]\n",
    "point2 = shapefile_cleaned.geometry.iloc[1]\n",
    "\n",
    "# Calculate the distance between the two points\n",
    "distance = point1.distance(point2)\n",
    "\n",
    "# Use half of the distance as the buffer distance\n",
    "buffer_distance = distance / 2\n",
    "\n",
    "# Apply the buffer to each point using the calculated buffer distance\n",
    "shapefile_cleaned['geometry'] = shapefile_cleaned.geometry.buffer(buffer_distance, cap_style=3)\n",
    "\n",
    "# Print the updated GeoDataFrame\n",
    "print(shapefile_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shapefile_cleaned.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "# Load the GeoDataFrame\n",
    "gdf = gpd.read_file('./inventory/landslides_liguria.shp')\n",
    "gdf['utc_date'] = pd.to_datetime(gdf['utc_date'], format=\"%d/%m/%Y %H:%M\")\n",
    "pointfile = gdf\n",
    "\n",
    "pointfile = pointfile.to_crs(shapefile_cleaned.crs)\n",
    "overlay_result = gpd.sjoin(shapefile_cleaned, pointfile, how='left', predicate='intersects')\n",
    "overlay_result['Landslide'] = overlay_result['index_right'].notnull().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "landslide_sum = overlay_result['Landslide'].sum()\n",
    "print(f\"Total Landslide overlaps: {landslide_sum}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(overlay_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_TH = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSI ranges based on PL_df\n",
    "lsi_values = np.arange(0.1, 1.1, 0.1)  \n",
    "PL_df = pd.DataFrame({'LSI': lsi_values})\n",
    "print(PL_df)\n",
    "lsi_ranges = PL_df['LSI'].tolist()\n",
    "lsi_bins = [0] + lsi_ranges  # Add 0 as the lower bound\n",
    "print(lsi_bins)\n",
    "\n",
    "# Filter rows where Landslide column is 1\n",
    "landslide_rows = overlay_result[overlay_result['Landslide'] == 1]\n",
    "\n",
    "# Bin the LSI02 values into the defined ranges for filtered rows\n",
    "landslide_rows['LSI_bin'] = pd.cut(landslide_rows['LSI02'], bins=lsi_bins, right=False)\n",
    "\n",
    "# Count the number of rows in each bin where Landslide is 1\n",
    "lsi_landslide_counts = landslide_rows['LSI_bin'].value_counts().sort_index()\n",
    "\n",
    "# Bin the LSI02 values into the defined ranges\n",
    "overlay_result['LSI_bin'] = pd.cut(overlay_result['LSI02'], bins=lsi_bins, right=False)\n",
    "\n",
    "# Count the total number of rows in each bin\n",
    "lsi_total_counts = overlay_result['LSI_bin'].value_counts().sort_index()\n",
    "\n",
    "# Combine both counts into a single DataFrame\n",
    "lsi_counts_df = pd.DataFrame({\n",
    "    'Landslide_Counts': lsi_landslide_counts,\n",
    "    'Total_Counts': lsi_total_counts\n",
    "}).fillna(0)  # Fill NaN values with 0\n",
    "\n",
    "print(lsi_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcultating the change in LSI due to rainfall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
